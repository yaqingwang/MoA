# MoA



We obtain result comparable or superior to full finetuning on the GLUE benchmark using [RoBERTa (Liu et al., 2019)](https://arxiv.org/abs/1907.11692)  large and [BERT ], while only training and storing a fraction of the parameters. Click the numbers below to download the RoBERTa and BERT MoA checkpoints.



<i>Note: You still need the original pre-trained checkpoint from [HuggingFace](https://huggingface.co/) to use the LoRA checkpoints.</i>


