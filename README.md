# MoA



We obtain result superior to full finetuning on the GLUE benchmark using [RoBERTa (Liu et al., 2019)](https://arxiv.org/abs/1907.11692)  large and [BERT (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805), while only training and storing a fraction of the parameters. Click the numbers below to download the RoBERTa and BERT MoA checkpoints.


<i>Note: You still need the original pre-trained checkpoint from [HuggingFace](https://huggingface.co/) to use the LoRA checkpoints.</i>


