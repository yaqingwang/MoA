# MoA



We obtain result superior to full finetuning on the GLUE benchmark using [RoBERTa (Liu et al., 2019)](https://arxiv.org/abs/1907.11692)  large and [BERT (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805), while only training and storing a fraction of the parameters. Please find RoBERTa and BERT [MoA checkpoints](https://github.com/yaqingwang/MoA/tree/master/examples/NLU).


<i>Note: You still need the original pre-trained checkpoint from [HuggingFace](https://huggingface.co/) to use the LoRA checkpoints.</i>


